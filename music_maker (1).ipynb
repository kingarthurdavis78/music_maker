{"cells":[{"cell_type":"markdown","metadata":{"id":"Cl_noqShIjNr"},"source":["# Final Project Part 2\n","\n","## Description\n","\n","Use diffusion to generate music\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YtG6fhcBIjNx","executionInfo":{"status":"ok","timestamp":1711326498091,"user_tz":360,"elapsed":10093,"user":{"displayName":"Arthur Hunter","userId":"13159643635799527441"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b8e38cd1-c337-4e37-f3dd-23d1a616efe0"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["from abc import abstractmethod\n","import math\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import AdamW\n","\n","import os\n","\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision\n","from torchvision.transforms import Compose, Resize, Lambda, ToTensor\n","from tqdm import tqdm\n","\n","device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jNr2K6q7IjNz"},"outputs":[],"source":["# Hyperparameters\n","MAX_TIMESTEPS = 2000\n","lr = .0001"]},{"cell_type":"code","source":["!unzip song_images.zip"],"metadata":{"id":"z-Bdr-u5YMVD"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nSeXhWSUIjN1"},"outputs":[],"source":["def zero_module(module):\n","    for p in module.parameters():\n","        p.detach().zero_()\n","    return module\n","\n","\n","def timestep_embedding(timesteps, dim, max_period=10000):\n","    half = dim // 2\n","    freqs = torch.exp(\n","        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n","    ).to(device=timesteps.device)\n","    args = timesteps[:, None].float() * freqs[None]\n","    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n","    if dim % 2:\n","        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n","    return embedding\n","\n","\n","class TimestepBlock(nn.Module):\n","    @abstractmethod\n","    def forward(self, x, emb):\n","        \"\"\"\n","        Apply the module to `x` given `emb` timestep embeddings.\n","        \"\"\"\n","\n","\n","class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n","    def forward(self, x, emb):\n","        for layer in self:\n","            if isinstance(layer, TimestepBlock):\n","                x = layer(x, emb)\n","            else:\n","                x = layer(x)\n","        return x\n","\n","\n","class Upsample(nn.Module):\n","    def __init__(self, channels):\n","        super().__init__()\n","        self.channels = channels\n","        self.conv = nn.Conv2d(channels, channels, 3, padding=1)\n","\n","    def forward(self, x):\n","        assert x.shape[1] == self.channels\n","        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n","        x = self.conv(x)\n","        return x\n","\n","\n","class Downsample(nn.Module):\n","    def __init__(self, channels):\n","        super().__init__()\n","        self.channels = channels\n","        self.op = nn.Conv2d(channels, channels, 3, stride=2, padding=1)\n","\n","    def forward(self, x):\n","        assert x.shape[1] == self.channels\n","        return self.op(x)\n","\n","\n","class ResBlock(TimestepBlock):\n","    def __init__(\n","        self,\n","        channels,\n","        emb_channels,\n","        dropout,\n","        out_channels=None,\n","    ):\n","        super().__init__()\n","        self.channels = channels\n","        self.emb_channels = emb_channels\n","        self.dropout = dropout\n","        self.out_channels = out_channels or channels\n","\n","        self.in_layers = nn.Sequential(\n","            nn.GroupNorm(128, channels),\n","            nn.SiLU(),\n","            nn.Conv2d(channels, self.out_channels, 3, padding=1)\n","        )\n","        self.emb_layers = nn.Sequential(\n","            nn.SiLU(),\n","            nn.Linear(emb_channels, self.out_channels)\n","        )\n","        self.out_layers = nn.Sequential(\n","            nn.GroupNorm(128, self.out_channels),\n","            nn.SiLU(),\n","            nn.Dropout(p=dropout),\n","            zero_module(nn.Conv2d(self.out_channels, self.out_channels, 3, padding=1)),\n","        )\n","\n","        if self.out_channels == channels:\n","            self.skip_connection = nn.Identity()\n","        else:\n","            self.skip_connection = nn.Conv2d(channels, self.out_channels, kernel_size=1)\n","\n","    def forward(self, x, emb):\n","        h = self.in_layers(x)\n","        emb_out = self.emb_layers(emb).type(h.dtype)\n","        while len(emb_out.shape) < len(h.shape):\n","            emb_out = emb_out[..., None]\n","        h = h + emb_out\n","        h = self.out_layers(h)\n","        return self.skip_connection(x) + h\n","\n","\n","class UNetModel(nn.Module):\n","    \"\"\"\n","    The full UNet model with timestep embedding.\n","\n","    :param in_channels: channels in the input Tensor.\n","    :param model_channels: base channel count for the model.\n","    :param out_channels: channels in the output Tensor.\n","    :param num_res_blocks: number of residual blocks per downsample.\n","    :param dropout: the dropout probability.\n","    :param channel_mult: channel multiplier for each level of the UNet.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        in_channels,\n","        hidden_channels,\n","        out_channels,\n","        num_res_blocks,\n","        dropout=0,\n","        channel_mult=(1, 2, 4, 8),\n","    ):\n","        super().__init__()\n","        self.in_channels = in_channels\n","        self.model_channels = hidden_channels\n","        self.out_channels = out_channels\n","        self.num_res_blocks = num_res_blocks\n","        self.dropout = dropout\n","        self.channel_mult = channel_mult\n","\n","        time_embed_dim = hidden_channels * 4\n","        self.time_embed = nn.Sequential(\n","            nn.Linear(hidden_channels, time_embed_dim),\n","            nn.SiLU(),\n","            nn.Linear(time_embed_dim, time_embed_dim),\n","        )\n","\n","        self.input_blocks = nn.ModuleList([\n","            TimestepEmbedSequential(nn.Conv2d(in_channels, hidden_channels, 3, padding=1))\n","        ])\n","        input_block_chans = [hidden_channels]\n","        ch = hidden_channels\n","        ds = 1\n","        for level, mult in enumerate(channel_mult):\n","            for _ in range(num_res_blocks):\n","                layers = [ResBlock(ch, time_embed_dim, dropout, out_channels=mult * hidden_channels)]\n","                ch = mult * hidden_channels\n","                self.input_blocks.append(TimestepEmbedSequential(*layers))\n","                input_block_chans.append(ch)\n","            if level != len(channel_mult) - 1:\n","                self.input_blocks.append(TimestepEmbedSequential(Downsample(ch)))\n","                input_block_chans.append(ch)\n","                ds *= 2\n","\n","        self.middle_block = TimestepEmbedSequential(\n","            ResBlock(ch, time_embed_dim, dropout),\n","            ResBlock(ch, time_embed_dim, dropout),\n","        )\n","\n","        self.output_blocks = nn.ModuleList([])\n","        for level, mult in list(enumerate(channel_mult))[::-1]:\n","            for i in range(num_res_blocks + 1):\n","                layers = [\n","                    ResBlock(\n","                        ch + input_block_chans.pop(),\n","                        time_embed_dim,\n","                        dropout,\n","                        out_channels=hidden_channels * mult,\n","                    )\n","                ]\n","                ch = hidden_channels * mult\n","                if level and i == num_res_blocks:\n","                    layers.append(Upsample(ch))\n","                    ds //= 2\n","                self.output_blocks.append(TimestepEmbedSequential(*layers))\n","\n","        self.out = nn.Sequential(\n","            nn.GroupNorm(128, ch),\n","            nn.SiLU(),\n","            zero_module(nn.Conv2d(hidden_channels, out_channels, 3, padding=1))\n","        )\n","\n","    def forward(self, x, timesteps):\n","        \"\"\"\n","        Apply the model to an input batch.\n","\n","        :param x: an [N x C x H x W] Tensor of inputs.\n","        :param timesteps: a 1-D batch of timesteps.\n","        :return: an [N x C x H x W] Tensor of outputs.\n","        \"\"\"\n","\n","        xs = []\n","        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n","\n","        for module in self.input_blocks:\n","            x = module(x, emb)\n","            xs.append(x)\n","        x = self.middle_block(x, emb)\n","        for module in self.output_blocks:\n","            cat_in = torch.cat([x, xs.pop()], dim=1)\n","            x = module(cat_in, emb)\n","        return self.out(x)\n"]},{"cell_type":"markdown","metadata":{"id":"LZrIrF1qIjN2"},"source":["# cosine scheduler\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fBuVxgKHIjN3"},"outputs":[],"source":["def cosine_beta_schedule(timesteps):\n","    \"\"\"\n","    Compute [β1,...,βT] using eq. 17 of \"Improved Denoising Diffusion Probabilistic Models\"\n","    \"\"\"\n","    s = 0.008\n","    # define f\n","    def f(x):\n","      return torch.cos((x / timesteps + s)/(1 + s) * 0.5 * torch.pi)**2\n","\n","    x = torch.linspace(0, timesteps, timesteps + 1)\n","    # calculate alpha\n","    alpha = f(x) / f(torch.tensor([0]))\n","    # calculate beta\n","    beta = 1 - alpha[1:] / alpha[:-1]\n","    beta = torch.clip(beta, 0.0001, 0.999)\n","    return beta\n"]},{"cell_type":"markdown","metadata":{"id":"SWvF4JsLDOFg"},"source":["Global variables for $\\beta$, $\\alpha$, and $\\bar{\\alpha}$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UPxewOfjIjN3"},"outputs":[],"source":["beta = cosine_beta_schedule(MAX_TIMESTEPS).to(device)\n","alpha = 1 - beta\n","alpha_bar = torch.cumprod(alpha, 0)"]},{"cell_type":"markdown","metadata":{"id":"2omNHgo8IjN4"},"source":["# the forward diffusion (training) process"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dWNOK2jXIjN4"},"outputs":[],"source":["def training(model, optimizer, dataloader, n_minibatches=3000):\n","\n","    losses = []\n","    model.train()  # Let's the model know it is in training model\n","\n","    batch_size = 20\n","\n","\n","    # Implement Algorithm 1 below. You can find what each symbol means in \"Denoising Diffusion Probabilistic Models\"\n","    # 1000-2000 minibatches (i.e. optimization steps) is sufficient for learning how to generate images, but more will help make better images\n","    # sample a batch of images\n","    for i in range(n_minibatches):\n","\n","      x0 = next(iter(dataloader))\n","      x0 = x0.to(device)\n","\n","\n","      # sample from the uniform distribution a batch of timesteps between 1 and T\n","      timesteps = torch.randint(1, MAX_TIMESTEPS, (batch_size,)).to(device)\n","\n","      # sample noise from the normal distribution (it needs to match the shape of the batch of images)\n","      noise = torch.randn_like(x0).to(device)\n","\n","      # create xt\n","      xt = torch.sqrt(alpha_bar[timesteps].view(-1, 1, 1, 1)) * x0 + torch.sqrt(1 - alpha_bar[timesteps].view(-1, 1, 1, 1)) * noise\n","\n","      et = model(xt, timesteps)\n","\n","      # calculate the loss\n","      loss = F.mse_loss(et, noise)\n","\n","      # take the gradient descent step\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","      losses.append(loss.item())\n","\n","\n","      print(f'{i} out of {n_minibatches}')\n","\n","    return model, losses\n"]},{"cell_type":"markdown","metadata":{"id":"ZdLhl2ldIjN5"},"source":["# the backward diffusion (sampling) process"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ggjhh3LiIjN5"},"outputs":[],"source":["@torch.no_grad()\n","def sampling(model, batch_size):\n","    model.eval()  # Let's the model know it is in validation mode\n","\n","    # Implement Algorithm 2 below. You can find what each symbol means in \"Denoising Diffusion Probabilistic Models\"\n","    shape = (batch_size, 1, 128, 128)\n","\n","    xt = torch.randn(shape, device=device)\n","\n","    for t in range(MAX_TIMESTEPS - 1, 0, -1):\n","        z = torch.randn(shape, device=device)\n","        x0_hat = (1 / (alpha_bar[t] ** 0.5)) * (xt - ((1 - alpha_bar[t]) ** 0.5) * model(xt, torch.tensor([t] * batch_size, device=device)))\n","        x0_hat = torch.clamp(x0_hat, -1, 1)\n","\n","        alpha_bar_minus = alpha_bar[t - 1] if t > 1 else 1\n","\n","        xt = (beta[t] * alpha_bar_minus ** 0.5) / (1 - alpha_bar[t]) * x0_hat + (((alpha[t] ** 0.5) * (1 - alpha_bar_minus)) / (1 - alpha_bar[t])) * xt\n","\n","        if t > 1:\n","            xt += (beta[t]**0.5) * z\n","\n","        if t % 100 == 0:\n","          print(f\"Sampling at t={t}\")\n","          plt.imshow(xt[0].cpu().numpy().squeeze(), cmap=\"gray\")\n","          plt.show()\n","\n","    return xt\n","\n"]},{"cell_type":"markdown","source":["# Custom DataLoader"],"metadata":{"id":"eMZr3JoV7cId"}},{"cell_type":"code","source":["class MidiDataset(Dataset):\n","    def __init__(self, folder_path):\n","        self.folder_path = folder_path\n","        self.file_list = os.listdir(folder_path)\n","\n","    def __len__(self):\n","        return len(self.file_list)\n","\n","    def __getitem__(self, idx):\n","        file_name = os.path.join(self.folder_path, self.file_list[idx])\n","        data = np.load(file_name)\n","        sample = torch.from_numpy(data['arr_0']).float().unsqueeze(0)\n","        if sample.shape != (1, 128, 128):\n","          return self.__getitem__(idx + 1)\n","        return sample"],"metadata":{"id":"EFntWanf7bnZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MOThW-hAIjN5"},"source":["# Part 4: Train and generate images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WbPHR9HoIjN5"},"outputs":[],"source":["model = UNetModel(\n","        in_channels=1,\n","        hidden_channels=128,\n","        out_channels=1,\n","        num_res_blocks=2,\n","        dropout=0.,\n","        channel_mult=(1, 2, 2, 2),\n","    ).to(device)\n","\n","batch_size = 20\n","dataset = MidiDataset(folder_path='song_images/')\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","optimizer = AdamW(model.parameters(), lr=lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lMA6Is7FIjN6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"762a4022-0301-4f87-f36c-4f0310db40f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["0 out of 2000\n","1 out of 2000\n","2 out of 2000\n","3 out of 2000\n","4 out of 2000\n","5 out of 2000\n","6 out of 2000\n","7 out of 2000\n","8 out of 2000\n","9 out of 2000\n","10 out of 2000\n","11 out of 2000\n","12 out of 2000\n","13 out of 2000\n","14 out of 2000\n","15 out of 2000\n","16 out of 2000\n","17 out of 2000\n","18 out of 2000\n","19 out of 2000\n","20 out of 2000\n","21 out of 2000\n","22 out of 2000\n","23 out of 2000\n","24 out of 2000\n","25 out of 2000\n","26 out of 2000\n","27 out of 2000\n","28 out of 2000\n","29 out of 2000\n","30 out of 2000\n","31 out of 2000\n","32 out of 2000\n","33 out of 2000\n","34 out of 2000\n","35 out of 2000\n","36 out of 2000\n","37 out of 2000\n","38 out of 2000\n","39 out of 2000\n","40 out of 2000\n","41 out of 2000\n","42 out of 2000\n","43 out of 2000\n","44 out of 2000\n","45 out of 2000\n","46 out of 2000\n","47 out of 2000\n","48 out of 2000\n","49 out of 2000\n","50 out of 2000\n","51 out of 2000\n","52 out of 2000\n","53 out of 2000\n","54 out of 2000\n","55 out of 2000\n","56 out of 2000\n","57 out of 2000\n","58 out of 2000\n","59 out of 2000\n","60 out of 2000\n","61 out of 2000\n","62 out of 2000\n","63 out of 2000\n","64 out of 2000\n","65 out of 2000\n","66 out of 2000\n","67 out of 2000\n","68 out of 2000\n","69 out of 2000\n","70 out of 2000\n","71 out of 2000\n","72 out of 2000\n","73 out of 2000\n","74 out of 2000\n","75 out of 2000\n","76 out of 2000\n","77 out of 2000\n","78 out of 2000\n","79 out of 2000\n","80 out of 2000\n","81 out of 2000\n","82 out of 2000\n","83 out of 2000\n","84 out of 2000\n","85 out of 2000\n","86 out of 2000\n","87 out of 2000\n","88 out of 2000\n","89 out of 2000\n","90 out of 2000\n","91 out of 2000\n","92 out of 2000\n","93 out of 2000\n","94 out of 2000\n","95 out of 2000\n","96 out of 2000\n","97 out of 2000\n","98 out of 2000\n","99 out of 2000\n","100 out of 2000\n","101 out of 2000\n","102 out of 2000\n","103 out of 2000\n","104 out of 2000\n","105 out of 2000\n","106 out of 2000\n","107 out of 2000\n","108 out of 2000\n","109 out of 2000\n","110 out of 2000\n","111 out of 2000\n","112 out of 2000\n","113 out of 2000\n","114 out of 2000\n","115 out of 2000\n","116 out of 2000\n","117 out of 2000\n","118 out of 2000\n","119 out of 2000\n","120 out of 2000\n","121 out of 2000\n","122 out of 2000\n","123 out of 2000\n","124 out of 2000\n","125 out of 2000\n","126 out of 2000\n","127 out of 2000\n","128 out of 2000\n","129 out of 2000\n","130 out of 2000\n","131 out of 2000\n","132 out of 2000\n","133 out of 2000\n","134 out of 2000\n","135 out of 2000\n","136 out of 2000\n","137 out of 2000\n","138 out of 2000\n","139 out of 2000\n","140 out of 2000\n","141 out of 2000\n","142 out of 2000\n","143 out of 2000\n","144 out of 2000\n","145 out of 2000\n","146 out of 2000\n","147 out of 2000\n","148 out of 2000\n","149 out of 2000\n","150 out of 2000\n","151 out of 2000\n","152 out of 2000\n","153 out of 2000\n","154 out of 2000\n","155 out of 2000\n","156 out of 2000\n","157 out of 2000\n","158 out of 2000\n","159 out of 2000\n","160 out of 2000\n","161 out of 2000\n","162 out of 2000\n","163 out of 2000\n","164 out of 2000\n","165 out of 2000\n","166 out of 2000\n","167 out of 2000\n","168 out of 2000\n","169 out of 2000\n","170 out of 2000\n","171 out of 2000\n","172 out of 2000\n","173 out of 2000\n","174 out of 2000\n","175 out of 2000\n","176 out of 2000\n","177 out of 2000\n","178 out of 2000\n","179 out of 2000\n","180 out of 2000\n","181 out of 2000\n","182 out of 2000\n","183 out of 2000\n","184 out of 2000\n","185 out of 2000\n","186 out of 2000\n","187 out of 2000\n","188 out of 2000\n","189 out of 2000\n","190 out of 2000\n","191 out of 2000\n","192 out of 2000\n","193 out of 2000\n","194 out of 2000\n","195 out of 2000\n","196 out of 2000\n","197 out of 2000\n","198 out of 2000\n","199 out of 2000\n","200 out of 2000\n","201 out of 2000\n","202 out of 2000\n","203 out of 2000\n","204 out of 2000\n","205 out of 2000\n","206 out of 2000\n","207 out of 2000\n","208 out of 2000\n","209 out of 2000\n","210 out of 2000\n","211 out of 2000\n","212 out of 2000\n","213 out of 2000\n","214 out of 2000\n","215 out of 2000\n","216 out of 2000\n","217 out of 2000\n"]}],"source":["model, losses = training(model, optimizer, dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tpUy2w-yDOFk"},"outputs":[],"source":["# plot your training loss\n","plt.plot(losses)\n","plt.title(\"Loss over Time\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k8LhTiPoIjN6"},"outputs":[],"source":["# Use sampling() to generate 16 images and plot them below\n","images = sampling(model, 1)"]},{"cell_type":"code","source":["img = images[0].cpu().squeeze().numpy()\n","img[img>0] = 1\n","img[img<=0] = -1\n","\n","plt.imshow(img, cmap='gray')\n","plt.axis('off')\n","plt.show()"],"metadata":{"id":"82yaBMunTEXJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! pip install pretty_midi"],"metadata":{"id":"bhq4Md621Au6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!sudo apt install -y fluidsynth\n","!pip install --upgrade pyfluidsynth"],"metadata":{"id":"xi6tjz_C3sjL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ilBAnp5aIjN7"},"source":["# Music"]},{"cell_type":"code","source":["import fluidsynth\n","import pretty_midi\n","from IPython import display\n","\n","SAMPLE_RATE = 16000"],"metadata":{"id":"qnK--6zYIEy3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def display_audio(pm: pretty_midi.PrettyMIDI, seconds=3):\n","  waveform = pm.fluidsynth(fs=SAMPLE_RATE)\n","  # Take a sample of the generated waveform to mitigate kernel resets\n","  waveform_short = waveform[:seconds*SAMPLE_RATE]\n","  return display.Audio(waveform_short, rate=SAMPLE_RATE)"],"metadata":{"id":"BQpK9ovdIL6E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def img_to_midi(img, filename):\n","  img[img == 1] = 127\n","  img[img == -1] = 0\n","  midi = pretty_midi.PrettyMIDI()\n","  piano = pretty_midi.Instrument(0)\n","  midi.instruments.append(piano)\n","  notes, frames = img.shape\n","\n","  prev_note = False\n","  start = 0\n","\n","  for n in range(notes):\n","      for f in range(frames):\n","          if img[n, f] > 0 and not prev_note:\n","              start = f\n","              prev_note = True\n","          elif img[n, f] <= 0 and prev_note:\n","              end = f\n","              piano.notes.append(pretty_midi.Note(velocity=100, pitch=n, start=start/10, end=end/10))\n","              prev_note = False\n","  midi.write(filename + '.mid')"],"metadata":{"id":"IOew4T49Nj2T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_to_midi(img, 'ai_midi')"],"metadata":{"id":"F3x0-uH61MIm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pm = pretty_midi.PrettyMIDI('ai_midi.mid')\n","display_audio(pm)"],"metadata":{"id":"sX7pbig71cuD"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}